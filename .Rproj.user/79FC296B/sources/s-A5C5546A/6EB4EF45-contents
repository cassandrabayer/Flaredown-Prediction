---
title: "Having fun with polls"
author: "Avi Feller"
output: html_document
---


# Load in 538 data
```{r}
library(readr)
library(dplyr)
dat <- read_csv("polls_538.csv")

```


We're only intersed in two-party vote share.
```{r}
dat <- dat %>%
          mutate(clinton_two_party = Clinton/(Clinton+Trump)) 
```


# Parametric simulation
What are the odds of Clinton winning Arizona? Let's simulate this!
```{r}
# set parameters
assumed_SE <- 0.03
n_sims <- 1000

# set seed
set.seed(8675309)

# Simulate vote share
Clinton_AZ_vote_share <- rnorm(n = n_sims, mean = dat$clinton_two_party[dat$State == "AZ"], sd = assumed_SE)

# What proportion are greater than 50%- which is what it would take it win 
MC_prob <- mean(Clinton_AZ_vote_share > 0.50)
MC_prob

library(ggplot2)
qplot(Clinton_AZ_vote_share, geom = "histogram") + geom_vline(xintercept = 0.50, col = "red")


```

How does this compare to a hypothesis test?


# Monte Carlo error
Let's compare this to the analytic version.
```{r}
#under the normal distribution, the odds that CLinton gets more than 50%
analytic_prob <- 1-pnorm(0.50, mean = dat$clinton_two_party[dat$State == "AZ"], sd = assumed_SE)
analytic_prob

#How far are we off?
MC_error <- MC_prob - analytic_prob
MC_error
#we're a little off, so we up the number of our simulations
```


What will the Monte Carlo error be if we crank up the number of simulations?
```{r}
n_sims <- 1000000
Clinton_AZ_vote_share <- rnorm(n = n_sims, mean = dat$clinton_two_party[dat$State == "AZ"], sd = assumed_SE)
MC_prob <- mean(Clinton_AZ_vote_share > 0.50)


MC_error <- MC_prob - analytic_prob
MC_error
#our error went down 
```



# Simulate an election

Let's repeat this same exercise for all states to calculate Electoral College votes. Notice that we set `assumed_SE` to be a default in the function.

```{r}
#the function takes in data, which is in the global environment, 
simulate_election <- function(assumed_SE = 0.03){

#above we did the simulation just for AZ, but now we want all of the states  
#we get the mean of the states in dat$clinton
#rnorm says to generated n samples, the mean is some number (in this case a vector), and the standard error is .03 which we've already generated
  clinton_vote_share <- rnorm(n = nrow(dat), mean = dat$clinton_two_party, sd = assumed_SE)
  clinton_EC <- sum(dat$EC_Votes[clinton_vote_share > 0.5]) #the number of her Electoral College votes are the sum of the number of states where her vote share was over 50%
  
  return(clinton_EC)
}

```



Run simulations. The `replicate` function is quite useful!
```{r}
n_sims <- 1000
simulated_EC <- replicate(n_sims, simulate_election())
```


Plot the output
```{r}
library(ggplot2)

qplot(simulated_EC, geom = "histogram", binwidth=4, fill=I("darkgrey"), col=I("white")) + 
    labs(x = "Simulated Electoral College Outcomes", y = "Number of Simulations", 
         title = "Simulated Electoral College Votes for Hilary Clinton") +
    xlim(200, 450) +
  theme_minimal() + 
  geom_vline(xintercept=270, col="red")
```


Based on our simulations, Hilary Clinton has a `r round(mean(simulated_EC > 270)*100)` probability of winning the election. Does that seem high or low? What should we change in our model?